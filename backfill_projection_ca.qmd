

```{r}
library(tidyverse)
library(epidatr)
library(epiprocess)
library(epipredict)
library(ggplot2)
library(plotly)

# At time of writing, epidatr doesn't handle caching of hosp facility data, and
# API doesn't have compactified issues. We'll do our own caching, and, to cut
# down on downloading, pretend that things are finalized after 60 days, except
# maybe to deal with the week redefinition.

analysis_date <- as.Date("2024-01-11")
stable_as_of <- analysis_date - 2L

ca_facilities_tbl <- pub_covid_hosp_facility_lookup(state = "ca")

# At least at state level, more regular reporting started around version
# 2021-09-21, with version 2021-09-21 a little different as well. So we'll start
# with the following version:
start_publication_date <- as.Date("2021-09-22")
end_publication_date <- stable_as_of
stopifnot(start_publication_date < end_publication_date)

max_lag = 120L

request_fields <- c(
  "hospital_pk", "state", "fips_code", "collection_week", "publication_date",
  "previous_day_admission_influenza_confirmed_7_day_sum" # no associated coverage stat?
)

cache <- cachem::cache_disk(dir = "imputation_facility_cache", max_size = 1 * 1024^3)
# XXX is this facility-list-change-proof? not to removals, but maybe those won't happen

# Normally to limit to versions >= some start version, we'd do an as_of query
# for the start version and an issues query for the rest (for efficiency). But
# for this endpoint, there are only sort of as_of queries (as_of queries except
# when querying on publication dates with no publication), so we'll use them for
# everything.

publication_dates <- seq(start_publication_date, end_publication_date, by="day")
chunks <- publication_dates %>%
  lapply(function(publication_date) {
    print(publication_date)
    cache_key <- format(publication_date, "publication_date_%Y%m%d")
    cache_chunk <- cache$get(cache_key)
    if (!cachem::is.key_missing(cache_chunk)) {
      chunk <- cache_chunk
    } else {
      print(system.time(
        chunk <-
          pub_covid_hosp_facility(
            hospital_pks = ca_facilities_tbl$hospital_pk,
            collection_weeks = epirange(publication_date - max_lag, publication_date),
            publication_dates = publication_date,
            fetch_args = fetch_args_list(
              fields = request_fields,
              timeout_seconds = 30L
            )
          )
      ))
      cache$set(cache_key, chunk)
    }
    chunk
  })

facilities_archive <- chunks %>%
  bind_rows() %>%
  rename(geo_value = hospital_pk, time_value = collection_week, version = publication_date,
         value = previous_day_admission_influenza_confirmed_7_day_sum) %>%
  # NOTE we're pretending we don't have to deal with censoring by replacing
  # -999999 with a fixed value (it appears to be a censoring of values in
  # [1..3] or some mix of [1..3] and [0..3] rather than [0..3] suggested by
  # docs; we'll replace it with 2, the average of [1..3])
  mutate(value = case_match(value, -999999 ~ 2, .default = value)) %>%
  as_epi_archive(compactify = TRUE)

counties_archive <- facilities_archive %>%
  epix_slide(
    before = max_lag,
    function(edf, gk, version) {
      edf %>%
        summarize(.by = c(fips_code, time_value), value = sum(value)) %>%
        # Missing rows were implicitly imputed to be 0 above. Do that explicitly
        # to make the latest time_value available the same for all counties:
        # complete(fips_code, time_value, fill = list(value = 0L), explicit = FALSE)
        #
        # also impute pre-existing NAs with 0s:
        complete(fips_code, time_value, fill = list(value = 0L))
    },
    as_list_col = TRUE
  ) %>%
  rename(version = time_value) %>%
  unnest(slide_value) %>%
  rename(geo_value = fips_code) %>%
  as_epi_archive(compactify = TRUE)

prototyping_versions <- publication_dates[
  publication_dates >= publication_dates[[1L]] + 26L*7L &
    seq_along(publication_dates) <= as.integer(0.7*length(publication_dates)) + 1L
] %>%
  `[`(as.POSIXlt(.)$wday == 3L)

max_analysis_lag <- max_lag

counties_version_lag_data <-
  counties_archive %>%
  epix_slide(
    ref_time_values = prototyping_versions,
    before = max_analysis_lag,
    ~ .x,
    as_list_col = TRUE
  ) %>%
  rename(version = time_value) %>%
  unnest(slide_value) %>%
  mutate(version_lag = as.integer(version - time_value))

counties_latest_snapshot <-
  counties_archive %>%
  epix_as_of(max(prototyping_versions))

county_means <- counties_latest_snapshot %>%
  summarize(.by = geo_value, county_mean = mean(value, na.rm = TRUE)) %>%
  mutate(county_mean_cat = cut(county_mean, unique(quantile(county_mean, (0:4)/4)), include.lowest = TRUE))

lags_and_latest <-
  left_join(counties_version_lag_data,
            counties_latest_snapshot %>%
              filter(time_value <= analysis_date - max_analysis_lag * 2L),
            by = c("geo_value", "time_value"),
            suffix = c("_lagged", "_latest")) %>%
  tidyr::drop_na(c(value_lagged, value_latest)) %>%
  left_join(county_means, by = "geo_value")

summarize_lag_evals <- function(grouped_lags_and_latest, by = c()) {
  grouped_lags_and_latest %>%
    summarize(
      .by = c({{by}}, version_lag),
      N = n(),
      evals = tibble(
        MAE = mean(abs(value_lagged - value_latest), na.rm = TRUE),
        MEnormlzd = mean(value_lagged - value_latest, na.rm = TRUE) / mean(value_latest) * 100,
        MAEnormlzd = mean(abs(value_lagged - value_latest), na.rm = TRUE) / mean(value_latest) * 100,
        Q1Enormlzd = quantile(value_lagged - value_latest, 0.25, na.rm = TRUE) / mean(value_latest) * 100,
        Q2Enormlzd = quantile(value_lagged - value_latest, 0.50, na.rm = TRUE) / mean(value_latest) * 100,
        Q3Enormlzd = quantile(value_lagged - value_latest, 0.75, na.rm = TRUE) / mean(value_latest) * 100,
        )
    )
}

plt <-
  lags_and_latest %>%
  summarize_lag_evals() %>%
  unpack(evals) %>%
  ggplot(aes(version_lag, MAEnormlzd)) +
  geom_line()
ggplotly(plt)

plt <-
  lags_and_latest %>%
  summarize_lag_evals() %>%
  unpack(evals) %>%
  ggplot(aes(version_lag, MEnormlzd)) +
  geom_line()
ggplotly(plt)

plt <-
  lags_and_latest %>%
  summarize_lag_evals(by = county_mean_cat) %>%
  unpack(evals) %>%
  ggplot(aes(version_lag, MAEnormlzd, colour = county_mean_cat)) +
  geom_line()
ggplotly(plt)

plt <-
  lags_and_latest %>%
  summarize_lag_evals(by = county_mean_cat) %>%
  unpack(evals) %>%
  ggplot(aes(version_lag, MEnormlzd, colour = county_mean_cat)) +
  geom_line()
ggplotly(plt)

plt <-
  lags_and_latest %>%
  summarize_lag_evals(by = geo_value) %>%
  unpack(evals) %>%
  # filter(N > 20L) %>%
  ggplot(aes(version_lag, MAEnormlzd, colour = geo_value, alpha = N)) +
  geom_line()
# ggplotly(plt)
plt

plt <-
  lags_and_latest %>%
  summarize_lag_evals(by = geo_value) %>%
  unpack(evals) %>%
  # filter(N > 20L) %>%
  ggplot(aes(version_lag, MEnormlzd, colour = geo_value, alpha = N)) +
  geom_line()
# ggplotly(plt)
plt

plt <-
  lags_and_latest %>%
  summarize_lag_evals(by = geo_value) %>%
  # filter(N > 20L) %>%
  mutate(evals = evals %>% select(starts_with("Q"))) %>%
  {pivot_longer(unpack(., evals), all_of(names(.$evals)))} %>%
  ggplot(aes(version_lag, value,
             group = interaction(geo_value, name),
             colour = geo_value,
             # linetype = name,
             alpha = N)) +
  geom_line() +
  geom_point(aes(shape = name),
             position = position_jitter(0.4, 1))
# ggplotly(plt)
plt

lags_and_latest %>%
  summarize_lag_evals(geo_value) %>%
  unpack(evals) %>%
  with(range(N))

```

Why is there a wide range of N values?
```{r}
lags_and_latest %>%
  summarize_lag_evals(geo_value) %>%
  slice_min(N, with_ties = FALSE) %>%
  select(geo_value) %>%
  left_join(counties_latest_snapshot, by = "geo_value") %>%
  summarize(N = n(), sum(!is.na(value)), sum(!is.na(value) & value != 0))

lags_and_latest %>%
  summarize_lag_evals(geo_value) %>%
  slice_max(N, with_ties = FALSE) %>%
  select(geo_value) %>%
  left_join(counties_latest_snapshot, by = "geo_value") %>%
  summarize(N = n(), sum(!is.na(value)), sum(!is.na(value) & value != 0))
```
